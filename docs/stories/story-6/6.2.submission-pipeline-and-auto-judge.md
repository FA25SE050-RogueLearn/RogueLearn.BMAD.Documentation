# Story 6.2: Code Battle — Submission Pipeline and Auto-Judge

## Status

Draft

## Story

As a Participant, I want to submit solutions during the Code Battle and receive automated judging results with scores and feedback, so that the competition is fair, transparent, and fast.

Reference: docs/stories/FA25_CP_grp4_RogueLearn_Review2-solution.md (Flow 3: Competition Process Steps 3–5)

## Acceptance Criteria

1. Participants can submit code files/snippets; the system timestamps and queues them for judging.
2. Auto-judge runs a test suite per problem and computes pass/fail and scores.
3. Per-submission results are notified to participants; final aggregate scores are maintained.
4. Rooms and capacity constraints are respected; check-in tokens unlock problem sets.
5. System resilience: retries failed judging tasks and prevents duplicate processing.
6. Metrics are captured: attempts per participant, pass rate by difficulty, average time per problem.

## Tasks / Subtasks

- [ ] Backend: Submission API and queueing; idempotency keys; storage for code artifacts.
- [ ] Backend: Auto-judge runner with configurable test cases; scoring logic per difficulty level.
- [ ] Backend: Metrics collection and analytics endpoints.
- [ ] Frontend: Room check-in flow; problem set display; submission UI and feedback.
- [ ] QA: Load tests for judging throughput; correctness tests; idempotency and retry scenarios.

## Dev Notes

- Entity References: submissions, problems, rooms, participant_assignments, notifications.
- Security: sandbox executions; resource limits; language restrictions per rules.
- Transparency: Show feedback details where safe; avoid revealing hidden tests.

## Change Log

| Date | Version | Description | Author |
| :--- | :--- | :--- | :--- |
| [Current Date] | 1.0 | Initial draft for submission and auto-judge | Dev Team |

## Dev Agent Record

### Agent Model Used
TBD

### Debug Log References
TBD

### Completion Notes List
TBD

### File List
TBD

## QA Results
TBD