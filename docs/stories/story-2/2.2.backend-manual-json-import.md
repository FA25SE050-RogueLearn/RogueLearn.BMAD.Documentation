# **Story 2.2: Backend - Manual Curriculum JSON Import and QuestLine Generation**

## Status

Done

## Ownership

*   **Primary Owner:** Minh Anh (Backend)
*   **Supporting:** An (Backend)
*   **Rationale:** This combines the data processing and quest generation logic into a single, straightforward backend task, making it a good fit for the core backend team.

## Story

**As a** Game Master (Admin),
**I want** to upload a pre-formatted JSON file containing curriculum data via an API endpoint,
**so that** the system can process it and automatically generate a new QuestLine.

## Acceptance Criteria

1. A new `POST /api/admin/curriculum/import` endpoint is created in the `QuestsService`.
2. The endpoint accepts a file upload (specifically, a `.json` file).
3. The service parses the JSON file, validating it against a predefined structure.
4. If validation passes, the service uses the data to create the corresponding `QuestLine`, `QuestChapters`, and `Quests` in the database, ordered by term/semester.
5. The endpoint returns the ID of the newly created `QuestLine`.
6. If the JSON is malformed or fails validation, the endpoint returns a `400 Bad Request` with clear error messages.
7. The endpoint is protected and only accessible by users with the 'Admin' role.

## Tasks / Subtasks

- [x] **Task 1: Define the Curriculum JSON Schema**
    - [x] In the `BuildingBlocks.Shared` project, create C# classes that define the expected structure of the curriculum JSON file.
- [x] **Task 2: Create the Import API Endpoint** (AC: #1, #2, #7)
    - [x] In the `QuestsService`, create a new `CurriculumImportController`.
    - [x] Implement the `POST /api/admin/curriculum/import` endpoint to handle a file upload.
    - [x] Secure the endpoint with the `[Authorize(Roles = "Admin")]` attribute.
- [x] **Task 3: Implement Parsing and Validation Logic** (AC: #3, #6)
    - [x] In the Application layer, create a `ImportCurriculumFromJsonCommand`.
    - [x] The handler for this command will be responsible for reading the uploaded file stream, deserializing it into the C# model, and validating its structure and content.
- [x] **Task 4: Implement QuestLine Generation Logic** (AC: #4)
    - [x] Create a `QuestGenerationService` that takes the validated curriculum data as input.
    - [x] This service will contain the logic to create the `QuestLine`, `QuestChapters`, and `Quest` entities based on the imported data. This re-incorporates the logic from the original, deleted Story 2.2.
- [x] **Task 5: Return Response** (AC: #5)
    - [x] Ensure the controller returns a `21 Created` status with the new `QuestLine` ID upon success.
- [x] **Task 6: Write Integration Tests** (AC: #3, #4, #5, #6)
    - [x] Write tests to verify that a valid JSON file successfully creates a complete and correctly structured QuestLine.
    - [x] Write tests to verify that invalid or malformed JSON files return appropriate `400` errors.

- [x] **Task 6a: AI-Powered Curriculum/Syllabus Import (Manual Text) via Semantic Kernel** (FR53)
    - [x] Implement `POST /api/admin/curriculum/import/text` accepting raw FLM text pasted by the Game Master.
    - [x] Add Semantic Kernel packages to the solution: `Microsoft.SemanticKernel` and (if using agents) `Microsoft.SemanticKernel.Agents.Core`. Configure connectors for Google Gemini using environment variables like `GEMINI_API_KEY` [0].
    - [x] Create an SK plugin/function (e.g., `CurriculumExtractor`) with prompt templates that normalize FLM text into structured JSON aligned with `extracted-data/` (e.g., `curriculum/*.json`, `syllabus/*.json`).
    - [x] Use SK’s structured output capabilities to enforce the JSON shape for programs, subjects, versions, structure mapping, and syllabi where applicable [0]. Validate and handle malformed outputs.
    - [x] Return the processed JSON in the response and make it available for downstream persistence.
    - [x] Enforce the 'Admin' role policy on this endpoint.

- [x] **Task 7: Define JSON Schema & Validation for Import**
    - [x] Specify the JSON structure expected for programs, subjects, curriculum versions, curriculum structure, and syllabus details (patterned after files in `extracted-data/`).
    - [x] Validate AI output against the schema; provide actionable validation errors.
    - [x] Include safeguards for missing fields, inconsistent codes, and term/semester mappings.

- [x] **Task 8: Implement CQRS Commands & Handlers for Persistence**
    - [x] Create commands and handlers to persist imported data: programs, subjects, curriculum versions, curriculum structure associations, and syllabus entries.
    - [x] Ensure idempotency/deduplication using natural keys (e.g., subject code, program code, version label) to avoid duplicate records across imports.
    - [x] Add query handlers to retrieve the imported entities for admin verification.      

- [x] **Task 9: Orchestrate Import Flow in Backend**
    - [x] Implement an orchestration handler/service that: calls SK extractor → validates JSON → dispatches CQRS commands to store data → aggregates results.
    - [x] Provide audit logs and metrics for imported item counts and any failures.
    - [x] Add error handling and partial failure reporting (e.g., when some subjects fail validation).

- [x] **Task 10: Extend Integration Tests for Import Flow** (AC: #7)
    - [x] Add tests that simulate an admin pasting raw FLM text and verify JSON transformation.
    - [x] Add tests for schema validation failures and success cases.
    - [x] Add tests for CQRS persistence, including idempotent re-import behavior.
    - [x] Verify non-admin access receives `403 Forbidden` for the import endpoint.

- [x] **Task 11: Semantic Kernel Configuration & Observability**
    - [x] Centralize SK kernel configuration, model selection, retry policies, and logging.
    - [x] Support switching between Azure OpenAI, OpenAI, and local providers (e.g., Ollama/LMStudio) for development flexibility [0].
    - [x] Document required environment variables and secrets management for deployment.

## Dev Notes

### **Architectural Guidance**
*   This story provides a manual on-ramp for curriculum data, decoupling us from the complexities of web scraping and AI for the MVP.
*   **Responsibility**: The `UserService` will own this entire workflow. It will not need to communicate with any other new services for this task.
*   **Human-in-the-Loop**: The "human" part of this loop is now at the very beginning: the Admin is responsible for preparing the clean JSON file. This simplifies the backend logic significantly.

### **Example JSON Structure to Support**
The system should expect a JSON file with the following structure. This should be formally defined in the C# models.

```json
{
  "curriculumCode": "BIT_SE_K19D_K20A",
  "name": "The Bachelor Program of Information Technology, Software Engineering Major",
  "programLearningOutcomes": [
    { "code": "PLO1", "description": "..." }
  ],
  "subjects": [
    {
      "code": "PRF192",
      "name": "Programming Fundamentals",
      "semester": 1,
      "credits": 3,
      "prerequisites": null
    },
    {
      "code": "PRO192",
      "name": "Object-Oriented Programming",
      "semester": 2,
      "credits": 3,
      "prerequisites": "Pass PRF192"
    }
  ]
}
```

## Change Log

| Date | Version | Description | Author |
| :--- | :--- | :--- | :--- |
| [Current Date] | 1.0 | Story redefined to focus on manual JSON import, deferring automated scraping and AI. | Winston, Architect |
